{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa5cf69-af37-49e5-9b53-063a19b2f3c5",
   "metadata": {},
   "source": [
    "เริ่มจากการโหลดโมเดลที่ต้องการเข้ามา พร้อมทดลองใช้งาน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b51af-5251-4295-b087-ed24c1e16fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "\n",
    "# Load the model\n",
    "llm = GPT4All(model='./mistral-7b-openorca.Q4_0.gguf')\n",
    "\n",
    "# Define a sentence to complete\n",
    "sentence = \"The capital of Thailand is \"\n",
    "\n",
    "# Complete the sentence\n",
    "response = llm.generate([sentence])\n",
    "print(response.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b4264-267a-47f2-8294-824ab4ac0af6",
   "metadata": {},
   "source": [
    "ในการสร้างผู้ช่วยส่วนตัว เราต้องเชื่อมต่อdataเข้ากับ LLM LlamaIndex ช่วยเราสำเร็จส่วนนี้ได้ด้วยโค้ดไม่กี่บรรทัด แต่ก่อนอื่น มาเตรียมembedding model และ sentence splitter เพื่อใช้ในกระบวนการเตรียมdataให้อยู่ในรูปแบบที่ง่ายต่อการใช้งานLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6fedc-207f-4d7c-a962-3e125e45dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core.indices.prompt_helper import PromptHelper\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# An embedding model used to structure text into representations\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "# PromptHelper can help deal with LLM context window and token limitations\n",
    "prompt_helper = PromptHelper(context_window=2048)\n",
    "\n",
    "# SentenceSplitter used to split our data into multiple chunks\n",
    "# Only a number of relevant chunks will be retrieved and fed into LLMs\n",
    "node_parser = SentenceSplitter(chunk_size=300, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a28a2-b4de-4947-8623-cbee206db97b",
   "metadata": {},
   "source": [
    "เราสามารถตั้ง global setting เพื่อใช้ส่วนประกอบทั้งหมดอย่าง llm, embed_model, prompt_helper, node_parser ดังโค้ดด้านล่างซึ่งมีประโยชน์ดังนี้:\n",
    "\n",
    "1.การตั้งค่าระดับ global: เราไม่ต้องกำหนดค่าเหล่านี้ในแต่ละที่ที่ใช้ LlamaIndex หรือส่วนประกอบต่างๆ ซึ่งทำให้โค้ดของเราดูสะอาดตาและจัดการได้ง่ายขึ้น\n",
    "2.ความยืดหยุ่น: เราสามารถปรับเปลี่ยนการตั้งค่าของระบบได้ในที่เดียว โดยไม่ต้องไปเปลี่ยนโค้ดที่ใช้แต่ละฟังก์ชันที่เกี่ยวข้อง\n",
    "3.การใช้ร่วมกันของหลายคอมโพเนนต์: เมื่อเรากำหนดค่าเหล่านี้ใน Settings คอมโพเนนต์ต่างๆ ที่ใช้ LlamaIndex เช่น VectorStoreIndex หรือ QueryEngine จะสามารถใช้การตั้งค่าเดียวกันได้โดยอัตโนมัติ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030f2a9-358d-46e8-9ed7-de9cccc2d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# Configure a global setting for our app so that the VectorStoreIndex and the QueryEngine can use these components by default.\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.prompt_helper = prompt_helper\n",
    "Settings.node_parser = node_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef8635-5cbd-4f9f-9950-67820787c6cc",
   "metadata": {},
   "source": [
    "ต่อไปเราจะโหลดdataเข้ามายังdocument ใช้service container เพื่อprocess document สร้างindexและเปลี่ยนindexให้เป็น query engine เพื่อที่เราจะใช้มันในการถามคำถาม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cef46-6d07-4153-9698-bbd627730cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Load data.txt into a document\n",
    "document = SimpleDirectoryReader(input_files=['./data.txt']).load_data()\n",
    "\n",
    "# Process data (chunking, embedding, indexing) and store them\n",
    "index = VectorStoreIndex.from_documents(document)\n",
    "\n",
    "# Build a query engine from the index\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664b5cd-c65d-4cb4-a8c2-36239f274d60",
   "metadata": {},
   "source": [
    "query engineนี้คือผู้ช่วยของเรา ซึ่งสามารถทำงานง่ายๆกับdataของเราได้ ตัวอย่างเช่น:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016ddbb-32c6-4728-9a1c-1993e4f430e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query('Give me my calendar.')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9090b3-3198-4507-87ed-9e7b31d56936",
   "metadata": {},
   "source": [
    "สำหรับการทำงานสรุปข้อความ ควรสร้างindexใหม่ที่เหมาะกับงานนี้ดีกว่า โดยใช้SummaryIndexซึ่งจะสรุปคำตอบจากทุกchunksของdataเปรียบเทียบกับ top-k chunksที่อยู่ใน VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c47fc-b1ab-4b25-b38c-dd3dbf8bf13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "\n",
    "# Load data.txt into a document\n",
    "document = SimpleDirectoryReader(input_files=['./data.txt']).load_data()\n",
    "\n",
    "# Process data (chunking, embedding, indexing) and store them\n",
    "summary_index = SummaryIndex.from_documents(document)\n",
    "\n",
    "# Build a summary engine from the index\n",
    "summary_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b542d-65e4-4466-a103-a4af894a0d11",
   "metadata": {},
   "source": [
    "ทดลองสรุปเนื้อหาข่าวดู"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b4fed-c504-4a20-aab3-f629a5002583",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = summary_engine.query(\"What is a summary of this collection of text?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
